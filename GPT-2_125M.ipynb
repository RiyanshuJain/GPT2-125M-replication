{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9Jt5UEaCTY5"
      },
      "source": [
        "## Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q9X47CGuyejt",
        "outputId": "ec0bae51-e66f-487e-a4e8-01aaf7f9610a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dataclasses import dataclass\n",
        "import inspect\n",
        "import math\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJh5hQmg_6VN"
      },
      "source": [
        "### GPT-2 model (125M) implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "V3-h3HES4L9g"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LTVfC-f5dXS"
      },
      "source": [
        "Multi-head Self-attention\n",
        "\n",
        "Attention(Q, K, V ) = softmax(Q @ K.T / √d_k) @ V (taken from \"Attention is All you Need\" paper)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "Qu-0BAI64WvC"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "\n",
        "        # check for Rotary Positional Embedding\n",
        "        if (config.emb_type_RoPE == True):\n",
        "            # print(\"Using RoPE\", end=' ')\n",
        "            self.rotary_emb = RotaryEmbedding(dim = config.n_embd // config.n_head)\n",
        "\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        # self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        self.flash = False\n",
        "        if not self.flash:\n",
        "            # print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        if (config.emb_type_RoPE == True):\n",
        "            # RoPE here\n",
        "            q, k = self.rotary_emb.rotate_queries_and_keys(q, k)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "GGb-TQyx42VV"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "\n",
        "        if (config.attn_type_GQA == True):\n",
        "            self.attn = GroupedQueryAttention(config)\n",
        "\n",
        "        else:\n",
        "            if (config.attn_type_SWA == True):\n",
        "                print(\"Sliding Window Attention not implemented yet. Using Self Attention by default !!!\")\n",
        "            self.attn = CausalSelfAttention(config)\n",
        "\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH90ssJ440lO"
      },
      "source": [
        "GPT-2 design of using both token and positional embeddings\n",
        "\n",
        "> (wte = nn.Embedding(config.vocab_size, config.n_embd) is the token embedding applied on Input to the network idx, the context vector U ==> W_e is self.transformer.wte(idx))\n",
        "\n",
        "> (wpe = nn.Embedding(config.block, config.n_embd) is the positional embedding applied on length of the Input to the network, pos = torch.arange(0, t, dtype=torch.long, device=device) ==> W_p is self.transformer.wpe(pos))\n",
        "\n",
        "> Both are added and passed on to the transformer block\n",
        "\n",
        "> h0 = U*W_e + W_p\n",
        "\n",
        "> h_l = transformer_block(h_{l−1})∀i ∈ [1, n]\n",
        "\n",
        "Transformer layers (n_layer=12) with multi-head (n_head=12) self-attention and position-wise feedforward network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "irx3FpGZ5s9d"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        if (config.emb_type_RoPE == True):\n",
        "            print(\"Using RoPE\\n\")\n",
        "\n",
        "        if (config.attn_type_GQA == True):\n",
        "            print(\"Using GQA\\n\")\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        # with weight tying when using torch.compile() some warnings get generated:\n",
        "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
        "        # This behavior is deprecated and will be an error in future versions\"\n",
        "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
        "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself, using GPT-2 design of using both token and positional embeddings here\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2qsxZJUlv0S"
      },
      "source": [
        "### Configuration for GPT-2 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "3z50JHFOUmLQ"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    # vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "    emb_type_RoPE: bool = False # for Rotatory Position Embedding\n",
        "    attn_type_GQA: bool = False # for Grouped Query attention\n",
        "    attn_type_SWA: bool = False # for Sliding Window attention\n",
        "    gqa_groups: int = 6 # number of groups for GQA\n",
        "    heads_per_group: int = 2 # heads/queries per group for GQA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODkeDMTQ8FB8"
      },
      "source": [
        "### Compile the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krIVoglj7UOS",
        "outputId": "371ac5cb-d205-4b90-a574-110c95dba0fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 123.65M\n",
            "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n"
          ]
        }
      ],
      "source": [
        "config = GPTConfig()\n",
        "model = GPT(config)\n",
        "model = model.to(device)\n",
        "optimizer = model.configure_optimizers(weight_decay=1e-2, learning_rate=1e-4, betas=(0.9, 0.95), device_type=device_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBSvBp7m8V_V"
      },
      "source": [
        "### Load the original GPT-2 125M model checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NroqCG8yU7u",
        "outputId": "daaaf1a1-6fbc-4225-f8ca-7e381880e09f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "odict_keys(['transformer.wte.weight', 'transformer.wpe.weight', 'transformer.h.0.ln_1.weight', 'transformer.h.0.ln_1.bias', 'transformer.h.0.attn.c_attn.weight', 'transformer.h.0.attn.c_attn.bias', 'transformer.h.0.attn.c_proj.weight', 'transformer.h.0.attn.c_proj.bias', 'transformer.h.0.ln_2.weight', 'transformer.h.0.ln_2.bias', 'transformer.h.0.mlp.c_fc.weight', 'transformer.h.0.mlp.c_fc.bias', 'transformer.h.0.mlp.c_proj.weight', 'transformer.h.0.mlp.c_proj.bias', 'transformer.h.1.ln_1.weight', 'transformer.h.1.ln_1.bias', 'transformer.h.1.attn.c_attn.weight', 'transformer.h.1.attn.c_attn.bias', 'transformer.h.1.attn.c_proj.weight', 'transformer.h.1.attn.c_proj.bias', 'transformer.h.1.ln_2.weight', 'transformer.h.1.ln_2.bias', 'transformer.h.1.mlp.c_fc.weight', 'transformer.h.1.mlp.c_fc.bias', 'transformer.h.1.mlp.c_proj.weight', 'transformer.h.1.mlp.c_proj.bias', 'transformer.h.2.ln_1.weight', 'transformer.h.2.ln_1.bias', 'transformer.h.2.attn.c_attn.weight', 'transformer.h.2.attn.c_attn.bias', 'transformer.h.2.attn.c_proj.weight', 'transformer.h.2.attn.c_proj.bias', 'transformer.h.2.ln_2.weight', 'transformer.h.2.ln_2.bias', 'transformer.h.2.mlp.c_fc.weight', 'transformer.h.2.mlp.c_fc.bias', 'transformer.h.2.mlp.c_proj.weight', 'transformer.h.2.mlp.c_proj.bias', 'transformer.h.3.ln_1.weight', 'transformer.h.3.ln_1.bias', 'transformer.h.3.attn.c_attn.weight', 'transformer.h.3.attn.c_attn.bias', 'transformer.h.3.attn.c_proj.weight', 'transformer.h.3.attn.c_proj.bias', 'transformer.h.3.ln_2.weight', 'transformer.h.3.ln_2.bias', 'transformer.h.3.mlp.c_fc.weight', 'transformer.h.3.mlp.c_fc.bias', 'transformer.h.3.mlp.c_proj.weight', 'transformer.h.3.mlp.c_proj.bias', 'transformer.h.4.ln_1.weight', 'transformer.h.4.ln_1.bias', 'transformer.h.4.attn.c_attn.weight', 'transformer.h.4.attn.c_attn.bias', 'transformer.h.4.attn.c_proj.weight', 'transformer.h.4.attn.c_proj.bias', 'transformer.h.4.ln_2.weight', 'transformer.h.4.ln_2.bias', 'transformer.h.4.mlp.c_fc.weight', 'transformer.h.4.mlp.c_fc.bias', 'transformer.h.4.mlp.c_proj.weight', 'transformer.h.4.mlp.c_proj.bias', 'transformer.h.5.ln_1.weight', 'transformer.h.5.ln_1.bias', 'transformer.h.5.attn.c_attn.weight', 'transformer.h.5.attn.c_attn.bias', 'transformer.h.5.attn.c_proj.weight', 'transformer.h.5.attn.c_proj.bias', 'transformer.h.5.ln_2.weight', 'transformer.h.5.ln_2.bias', 'transformer.h.5.mlp.c_fc.weight', 'transformer.h.5.mlp.c_fc.bias', 'transformer.h.5.mlp.c_proj.weight', 'transformer.h.5.mlp.c_proj.bias', 'transformer.h.6.ln_1.weight', 'transformer.h.6.ln_1.bias', 'transformer.h.6.attn.c_attn.weight', 'transformer.h.6.attn.c_attn.bias', 'transformer.h.6.attn.c_proj.weight', 'transformer.h.6.attn.c_proj.bias', 'transformer.h.6.ln_2.weight', 'transformer.h.6.ln_2.bias', 'transformer.h.6.mlp.c_fc.weight', 'transformer.h.6.mlp.c_fc.bias', 'transformer.h.6.mlp.c_proj.weight', 'transformer.h.6.mlp.c_proj.bias', 'transformer.h.7.ln_1.weight', 'transformer.h.7.ln_1.bias', 'transformer.h.7.attn.c_attn.weight', 'transformer.h.7.attn.c_attn.bias', 'transformer.h.7.attn.c_proj.weight', 'transformer.h.7.attn.c_proj.bias', 'transformer.h.7.ln_2.weight', 'transformer.h.7.ln_2.bias', 'transformer.h.7.mlp.c_fc.weight', 'transformer.h.7.mlp.c_fc.bias', 'transformer.h.7.mlp.c_proj.weight', 'transformer.h.7.mlp.c_proj.bias', 'transformer.h.8.ln_1.weight', 'transformer.h.8.ln_1.bias', 'transformer.h.8.attn.c_attn.weight', 'transformer.h.8.attn.c_attn.bias', 'transformer.h.8.attn.c_proj.weight', 'transformer.h.8.attn.c_proj.bias', 'transformer.h.8.ln_2.weight', 'transformer.h.8.ln_2.bias', 'transformer.h.8.mlp.c_fc.weight', 'transformer.h.8.mlp.c_fc.bias', 'transformer.h.8.mlp.c_proj.weight', 'transformer.h.8.mlp.c_proj.bias', 'transformer.h.9.ln_1.weight', 'transformer.h.9.ln_1.bias', 'transformer.h.9.attn.c_attn.weight', 'transformer.h.9.attn.c_attn.bias', 'transformer.h.9.attn.c_proj.weight', 'transformer.h.9.attn.c_proj.bias', 'transformer.h.9.ln_2.weight', 'transformer.h.9.ln_2.bias', 'transformer.h.9.mlp.c_fc.weight', 'transformer.h.9.mlp.c_fc.bias', 'transformer.h.9.mlp.c_proj.weight', 'transformer.h.9.mlp.c_proj.bias', 'transformer.h.10.ln_1.weight', 'transformer.h.10.ln_1.bias', 'transformer.h.10.attn.c_attn.weight', 'transformer.h.10.attn.c_attn.bias', 'transformer.h.10.attn.c_proj.weight', 'transformer.h.10.attn.c_proj.bias', 'transformer.h.10.ln_2.weight', 'transformer.h.10.ln_2.bias', 'transformer.h.10.mlp.c_fc.weight', 'transformer.h.10.mlp.c_fc.bias', 'transformer.h.10.mlp.c_proj.weight', 'transformer.h.10.mlp.c_proj.bias', 'transformer.h.11.ln_1.weight', 'transformer.h.11.ln_1.bias', 'transformer.h.11.attn.c_attn.weight', 'transformer.h.11.attn.c_attn.bias', 'transformer.h.11.attn.c_proj.weight', 'transformer.h.11.attn.c_proj.bias', 'transformer.h.11.ln_2.weight', 'transformer.h.11.ln_2.bias', 'transformer.h.11.mlp.c_fc.weight', 'transformer.h.11.mlp.c_fc.bias', 'transformer.h.11.mlp.c_proj.weight', 'transformer.h.11.mlp.c_proj.bias', 'transformer.ln_f.weight', 'transformer.ln_f.bias', 'lm_head.weight'])\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "model_hf = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "sd_hf = model_hf.state_dict()\n",
        "print(sd_hf.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WRNzZZX8ZVk"
      },
      "source": [
        "### Copy the loaded weights in our implementation of GPT-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPHsPXaK2wj4",
        "outputId": "b6175e69-0073-48b3-c944-e0513a8f2c6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrueTrue"
          ]
        }
      ],
      "source": [
        "transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "for i in sd_hf:\n",
        "    if any(i.endswith(w) for w in transposed):\n",
        "        print(True, end='')\n",
        "        sd_hf[i] = sd_hf[i].t()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "npN9W7gjzLgm"
      },
      "outputs": [],
      "source": [
        "model_state = model.state_dict()\n",
        "for i in model_state.keys():\n",
        "    if not i.endswith('.attn.bias'):\n",
        "        model_state[i] = sd_hf[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A-mdZxr0QNp",
        "outputId": "bcd21eed-9a8d-4337-d853-8158a46eb4c0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(model_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXhXgeIf8oEE"
      },
      "source": [
        "### Run a sample prediction with the original weights combined with our implementation to validate the correct working of created model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt7bmO7dNLQF",
        "outputId": "ec57f607-fb77-426e-be1e-d419935649b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dP1zuQS74lj",
        "outputId": "73cf70b2-0281-4e56-890c-78848878288d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the best way to get to your position without being forced by your opponents to fight the same fights around you in\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "text = '''What is the best'''\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "# print(encoded_input)\n",
        "outputs = model.generate(idx = encoded_input['input_ids'].to(device), max_new_tokens = 20, temperature = 0.8, top_k = 200)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMLmTcx2nIKI"
      },
      "source": [
        "### References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkTRSQAQvXMo"
      },
      "source": [
        "1. Attention research paper - https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
        "\n",
        "2. GPT research paper - https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\n",
        "\n",
        "3. GPT2 research paper - https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
        "\n",
        "4. Github used - https://github.com/karpathy/nanoGPT/blob/master/model.py#L6\n",
        "\n",
        "5. Huggingface resources used - https://huggingface.co/gpt2\n",
        "https://huggingface.co/transformers/v3.0.2/_modules/transformers/tokenization_gpt2.html#GPT2Tokenizer\n",
        "https://discuss.huggingface.co/t/how-to-decode-gpt2/16160"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvzAlVnvDbX8"
      },
      "source": [
        "## Task 3 - requires user to give input (sGPU, DDP, FSDP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2LWds3xFeRG"
      },
      "source": [
        "### Prepare dataset for training (using tinyshakespeare/input.txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWSajX27FnuW"
      },
      "source": [
        "Reference link for dataset -- https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69rPVKYHJykv",
        "outputId": "d94dafac-26f8-4f2c-e4cd-c7f5ff6ca129"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "wIDsjNIQTuHd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import requests\n",
        "import tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPMirJRXyYY1",
        "outputId": "a15c9033-67b5-4d24-ea81-d58363a6ddf6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train has 301,966 tokens\n",
            "train has 36,059 tokens\n"
          ]
        }
      ],
      "source": [
        "data_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "input_file_path = './data.txt'\n",
        "with open(input_file_path, 'w') as f:\n",
        "    f.write(requests.get(data_url).text)\n",
        "with open(input_file_path, 'r') as f:\n",
        "    data = f.read()\n",
        "n = len(data)\n",
        "train_data = data[:int(n*0.9)]\n",
        "val_data = data[int(n*0.9):]\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "train_ids = enc.encode_ordinary(train_data)\n",
        "print(f\"train has {len(train_ids):,} tokens\")\n",
        "val_ids = enc.encode_ordinary(val_data)\n",
        "print(f\"train has {len(val_ids):,} tokens\")\n",
        "\n",
        "train_file_path = './train.bin'\n",
        "val_file_path = './val.bin'\n",
        "# export to bin files\n",
        "train_ids = np.array(train_ids, dtype=np.int16)\n",
        "val_ids = np.array(val_ids, dtype=np.int16)\n",
        "train_ids.tofile(train_file_path)\n",
        "val_ids.tofile(val_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q9eQUUv3xx2",
        "outputId": "2af39ff3-531e-46bf-f68b-7ceb74c6f80d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1003854\n",
            "301966\n",
            "F 5962\n"
          ]
        }
      ],
      "source": [
        "print((len(train_data)))\n",
        "print(len(train_ids))\n",
        "print(train_data[0], train_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsOwOG7HFyUh"
      },
      "source": [
        "We have saved the training and validation files in \"train.bin\" and \"val.bin\" in the current directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGHdRX7gS0CO"
      },
      "source": [
        "### Training function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "rXPOZ7B9Hdc5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch.distributed as dist\n",
        "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "\n",
        "def setup(rank, world_size):\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "\n",
        "    # initialize the process group\n",
        "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
        "\n",
        "def cleanup():\n",
        "    dist.destroy_process_group()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "u6xdZSEXwKo7"
      },
      "outputs": [],
      "source": [
        "def get_batch(data, device_type, batch_size = 2):\n",
        "    block_size = config.block_size\n",
        "    ix = torch.randint(len(data) - config.block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device_type == 'cuda':\n",
        "        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "kFKcQ0WY221n"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model, train_data, val_data, device_type, batch_size = 2):\n",
        "    out = {}\n",
        "    eval_iters = 20\n",
        "    model.eval()\n",
        "    split = 'train'\n",
        "    for data in [train_data, val_data]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(data, device_type, batch_size = 2)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean() / batch_size\n",
        "        split = 'val'\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "lrHvk4MzItZd"
      },
      "outputs": [],
      "source": [
        "def train_loop(rank, world_size, partial_epochs, iters, train_data, device_type, optimizer, model, val_data, mode='sGPU', batch_size = 2):\n",
        "    if (mode == 'sGPU'):\n",
        "        pass\n",
        "    elif (mode == 'DDP'):\n",
        "        setup(rank, world_size)\n",
        "        model = model.to(rank)\n",
        "        ddp_model = DDP(model, device_ids=[rank], find_unused_parameters=True)\n",
        "        optimizer = torch.optim.AdamW(ddp_model.parameters(), weight_decay=1e-2, lr=1e-4, betas=(0.9, 0.95))\n",
        "    elif (mode == 'FSDP'):\n",
        "        setup(rank, world_size)\n",
        "        model = model.to(rank)\n",
        "        sampler = DistributedSampler(train_data, rank=rank, num_replicas=world_size, shuffle=True)\n",
        "        train_kwargs = {'batch_size': batch_size, 'sampler': sampler, 'num_workers': 2,\n",
        "                    'pin_memory': True,\n",
        "                    'shuffle': False}\n",
        "        train_loader = torch.utils.data.DataLoader(train_data,**train_kwargs)\n",
        "        torch.cuda.set_device(rank)\n",
        "        fsdp_model = FSDP(model)\n",
        "        optimizer = torch.optim.AdamW(fsdp_model.parameters(), weight_decay=1e-2, lr=1e-4, betas=(0.9, 0.95))\n",
        "\n",
        "    # common for all\n",
        "    for epoch in range(1, partial_epochs+1):\n",
        "        losses = 0\n",
        "        total = 0\n",
        "\n",
        "        if (mode == 'FSDP'):\n",
        "            ddp_loss = torch.zeros(2).to(rank)\n",
        "            sampler.set_epoch(epoch)\n",
        "\n",
        "        for iter in tqdm(range(iters)):\n",
        "            X, Y = get_batch(train_data, device_type, batch_size = 2)\n",
        "\n",
        "            if (mode == 'sGPU'):\n",
        "                X, Y = X.to(device), Y.to(device)\n",
        "            else:\n",
        "                X, Y = X.to(rank), Y.to(rank)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if (mode == 'sGPU'):\n",
        "                logits, loss = model(X, Y)\n",
        "            elif (mode == 'DDP'):\n",
        "                logits, loss = ddp_model(X, Y)\n",
        "            elif (mode == 'FSDP'):\n",
        "                logits, loss = fsdp_model(X, Y)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if (mode != 'FSDP'):\n",
        "                  losses += loss.item()\n",
        "                  total += len(X)\n",
        "            else:\n",
        "                ddp_loss[0] += loss.item()\n",
        "                ddp_loss[1] += len(X)\n",
        "\n",
        "        if (mode == 'FSDP'):\n",
        "            dist.all_reduce(ddp_loss, op=dist.ReduceOp.SUM)\n",
        "\n",
        "        print(f\"Epoch:{epoch}, Loss:{losses/total}\" if (mode != 'FSDP') else f\"Epoch:{epoch}, Loss:{ddp_loss[0] / ddp_loss[1]}\")\n",
        "        print(estimate_loss(model, train_data, val_data, device_type, batch_size = 2))\n",
        "        print(\"------------------------\\n\")\n",
        "\n",
        "    if (mode != 'sGPU'):\n",
        "        cleanup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "HJvnLUp40s0z"
      },
      "outputs": [],
      "source": [
        "def train(train_data, val_data, device_type, mode='sGPU'):\n",
        "    '''\n",
        "      compatible value of mode = sGPU, DDP, and FSDP\n",
        "    '''\n",
        "    assert mode == 'sGPU' or mode == 'DDP' or mode == 'FSDP', \"compatible value of mode = sGPU, DDP, and FSDP\"\n",
        "\n",
        "    partial_epochs = 5\n",
        "    iters = 20\n",
        "    batch_size = 2\n",
        "    config = GPTConfig()\n",
        "    model = GPT(config)\n",
        "    n_gpus = torch.cuda.device_count()\n",
        "    world_size = n_gpus\n",
        "    rank = 0\n",
        "\n",
        "    if (mode == 'sGPU'):\n",
        "        model = model.to(device)\n",
        "        optimizer = model.configure_optimizers(weight_decay=1e-2, learning_rate=1e-4, betas=(0.9, 0.95), device_type=device_type)\n",
        "        train_loop(rank, world_size, partial_epochs, iters, train_data, device_type, optimizer, model, val_data, mode, batch_size)\n",
        "\n",
        "    else: # (mode == 'DDP' or mode == 'FSDP'):\n",
        "        if n_gpus < 2:\n",
        "            print(f\"Requires at least 2 GPUs to run Multiprocessing using DDP or FSDP, but got {n_gpus}\")\n",
        "            train_loop(rank, world_size, partial_epochs, iters, train_data, device_type, None, model, val_data, mode, batch_size)\n",
        "        else:\n",
        "            print(f\"Utilizing {n_gpus} GPUs\\n\")\n",
        "            mp.spawn(train_loop,\n",
        "                args=(world_size, partial_epochs, iters, train_data, device_type, None, model, val_data, mode, batch_size,),\n",
        "                nprocs=world_size,\n",
        "                join=True)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbIdKX7B04GJ"
      },
      "source": [
        "### Training on the prepared data - give input by expanding this block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRqbGOKiETN6",
        "outputId": "d086d4d6-6008-4d81-d8ce-3b78888afe40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter the mode in which you want to train the file (compatible value of mode = sGPU, DDP, and FSDP)FSDP\n",
            "number of parameters: 123.65M\n",
            "Requires at least 2 GPUs to run Multiprocessing using DDP or FSDP, but got 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:12<00:00,  1.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Loss:4.34592342376709\n",
            "{'train': tensor(3.8753), 'val': tensor(3.8435)}\n",
            "------------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:13<00:00,  1.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Loss:3.586996078491211\n",
            "{'train': tensor(3.3317), 'val': tensor(3.3227)}\n",
            "------------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:13<00:00,  1.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:3, Loss:3.2246177196502686\n",
            "{'train': tensor(3.0996), 'val': tensor(3.1252)}\n",
            "------------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:12<00:00,  1.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:4, Loss:3.0579822063446045\n",
            "{'train': tensor(3.0534), 'val': tensor(3.0596)}\n",
            "------------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:13<00:00,  1.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:5, Loss:3.0134801864624023\n",
            "{'train': tensor(2.9528), 'val': tensor(3.0299)}\n",
            "------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap('val.bin', dtype=np.uint16, mode='r')\n",
        "while (True):\n",
        "    mode = input(\"Enter the mode in which you want to train the file (compatible value of mode = sGPU, DDP, and FSDP)\")\n",
        "    if (mode == 'sGPU' or mode == 'DDP' or mode == 'FSDP'):\n",
        "        break;\n",
        "    else:\n",
        "        print(\"compatible value of mode = sGPU, DDP, and FSDP\")\n",
        "\n",
        "# call training function with mode specified\n",
        "model = train(train_data, val_data, device_type, mode=mode)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v62hNdI0RMZ"
      },
      "source": [
        "If you encounter \"RuntimeError: trying to initialize the default process group twice!\", run this cell by uncommenting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "HP7Daqmx0O8X"
      },
      "outputs": [],
      "source": [
        "# cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpbmPYN9GE-Z"
      },
      "source": [
        "Generate a prediction after training on the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLcZZMOgqePK",
        "outputId": "02d3006c-40a8-4928-e1d6-f260c2f84803"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the role\n",
            "\n",
            "\n",
            "GLES:\n",
            "\n",
            " of hear'll?\n",
            "\n",
            "As her off in a like\n"
          ]
        }
      ],
      "source": [
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "text = '''What is the role'''\n",
        "outputs = model.generate(idx = torch.tensor([enc.encode_ordinary(text)]).to(device), max_new_tokens = 20, temperature = 0.8, top_k = 200)\n",
        "print(enc.decode(outputs[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJi01WrZ1CUb"
      },
      "source": [
        "### References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60W2JlPT1E-5"
      },
      "source": [
        "DDP - https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\n",
        "\n",
        "FSDP - https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html#how-to-use-fsdp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ye5rSNX6bYu"
      },
      "source": [
        "Unused code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "43t0ygp26cKG"
      },
      "outputs": [],
      "source": [
        "# from tqdm import tqdm\n",
        "# def train(model, optimizer, train_data, val_data, device_type):\n",
        "#     partial_epochs = 1\n",
        "#     iters = 50\n",
        "#     train_kwargs = {'batch_size': batch_size, 'num_workers': 2,\n",
        "#                     'pin_memory': True,\n",
        "#                     'shuffle': False}\n",
        "#     train_loader = torch.utils.data.DataLoader(train_data,**train_kwargs)\n",
        "\n",
        "#     for epoch in range(1, partial_epochs+1):\n",
        "#         progress = tqdm(enumerate(train_loader), desc=\"Epoch: {}\".format(epoch), total=len(train_loader))\n",
        "#         loss = 0\n",
        "#         total = 0\n",
        "#         for iter, (X, Y) in progress:\n",
        "#             X, Y = X.to(device), Y.to(device)\n",
        "#             print(X)\n",
        "#             optimizer.zero_grad()\n",
        "#             logits, loss = model(X, Y)\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "#             progress.update(1)\n",
        "#             loss += loss.item()\n",
        "#             total += len(X)\n",
        "#         print(f\"Epoch{epoch}, Loss:{loss/total}\")\n",
        "\n",
        "# train_data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
        "# val_data = np.memmap('val.bin', dtype=np.uint16, mode='r')\n",
        "# train(model, optimizer, train_data, val_data, device_type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHTDIX1Z1eHL"
      },
      "source": [
        "## Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZDWYOSy3FRZ"
      },
      "source": [
        "### RoPE (Rotary Positional Embedding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RRZKwjW1dZZ",
        "outputId": "6fd0b063-aae3-4d3c-afeb-e113c2281e91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ],
      "source": [
        "pip install einops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEMohZ5xTJAV"
      },
      "source": [
        "#### Defining helper functions and Class to apply Rotary Positional Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-ikHMEXDT1RY"
      },
      "outputs": [],
      "source": [
        "from math import pi, log\n",
        "\n",
        "import torch\n",
        "from torch.cuda.amp import autocast\n",
        "from torch import einsum, broadcast_tensors\n",
        "\n",
        "from einops import rearrange, repeat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "W939MfxhMzz1"
      },
      "outputs": [],
      "source": [
        "# helper functions\n",
        "\n",
        "def exists(val):\n",
        "    return val is not None\n",
        "\n",
        "def default(val, d):\n",
        "    return val if exists(val) else d\n",
        "\n",
        "# rotary embedding helper functions\n",
        "\n",
        "def rotate_half(x):\n",
        "    x = rearrange(x, '... (d r) -> ... d r', r = 2)\n",
        "    x1, x2 = x.unbind(dim = -1)\n",
        "    x = torch.stack((-x2, x1), dim = -1)\n",
        "    return rearrange(x, '... d r -> ... (d r)')\n",
        "\n",
        "@autocast(enabled = False)\n",
        "def apply_rotary_emb(freqs, t, start_index = 0, scale = 1., seq_dim = -2):\n",
        "    rot_dim, seq_len = freqs.shape[-1], t.shape[seq_dim]\n",
        "    freqs = freqs[-seq_len:].to(t)\n",
        "\n",
        "    end_index = start_index + rot_dim\n",
        "    assert rot_dim <= t.shape[-1], f'feature dimension {t.shape[-1]} is not of sufficient size to rotate in all the positions {rot_dim}'\n",
        "    t_left, t, t_right = t[..., :start_index], t[..., start_index:end_index], t[..., end_index:]\n",
        "    t = (t * freqs.cos() * scale) + (rotate_half(t) * freqs.sin() * scale)\n",
        "    return torch.cat((t_left, t, t_right), dim = -1)\n",
        "\n",
        "# learned rotation helpers\n",
        "\n",
        "def apply_learned_rotations(rotations, t, start_index = 0, freq_ranges = None):\n",
        "    if exists(freq_ranges):\n",
        "        rotations = einsum('..., f -> ... f', rotations, freq_ranges)\n",
        "        rotations = rearrange(rotations, '... r f -> ... (r f)')\n",
        "\n",
        "    rotations = repeat(rotations, '... n -> ... (n r)', r = 2)\n",
        "    return apply_rotary_emb(rotations, t, start_index = start_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "OunyzkmyT8be"
      },
      "outputs": [],
      "source": [
        "# classes\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        theta = 10000,\n",
        "        max_freq = 10,\n",
        "        num_freqs = 1,\n",
        "        learned_freq = False,\n",
        "        use_xpos = True,\n",
        "        xpos_scale_base = 512,\n",
        "        interpolate_factor = 1.,\n",
        "        theta_rescale_factor = 1.,\n",
        "        seq_before_head_dim = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # proposed by reddit user bloc97, to rescale rotary embeddings to longer sequence length without fine-tuning\n",
        "        # has some connection to NTK literature\n",
        "        # https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/\n",
        "\n",
        "        # theta *= theta_rescale_factor ** (dim / (dim - 2))\n",
        "\n",
        "        # θi = 10000**(-2i/d)\n",
        "        freqs = 1. / (theta ** (torch.arange(0, dim, 2)[:(dim // 2)].float() / dim))\n",
        "\n",
        "        self.cache = dict()\n",
        "        self.cache_scale = dict()\n",
        "        self.freqs = nn.Parameter(freqs, requires_grad = learned_freq)\n",
        "\n",
        "        self.learned_freq = learned_freq\n",
        "\n",
        "        # default sequence dimension\n",
        "\n",
        "        self.seq_before_head_dim = seq_before_head_dim\n",
        "        self.default_seq_dim = -3 if seq_before_head_dim else -2\n",
        "\n",
        "        # interpolation factors\n",
        "\n",
        "        assert interpolate_factor >= 1.\n",
        "        self.interpolate_factor = interpolate_factor\n",
        "\n",
        "        # xpos\n",
        "\n",
        "        self.use_xpos = use_xpos\n",
        "        if not use_xpos:\n",
        "            self.register_buffer('scale', None)\n",
        "            return\n",
        "\n",
        "        scale = (torch.arange(0, dim, 2) + 0.4 * dim) / (1.4 * dim)\n",
        "        self.scale_base = xpos_scale_base\n",
        "        self.register_buffer('scale', scale)\n",
        "\n",
        "    def get_seq_pos(self, seq_len, device, dtype, offset = 0):\n",
        "        return (torch.arange(seq_len, device = device, dtype = dtype) + offset) / self.interpolate_factor\n",
        "\n",
        "    def rotate_queries_and_keys(self, q, k, seq_dim = None):\n",
        "        seq_dim = default(seq_dim, self.default_seq_dim)\n",
        "\n",
        "        assert self.use_xpos\n",
        "        device, dtype, seq_len = q.device, q.dtype, q.shape[seq_dim]\n",
        "\n",
        "        seq = self.get_seq_pos(seq_len, dtype = dtype, device = device)\n",
        "        freqs = self.forward(lambda: seq, cache_key = f'freqs:{seq_len}')\n",
        "        scale = self.get_scale(lambda: seq, cache_key = f'scale:{seq_len}').to(dtype)\n",
        "\n",
        "        if seq_dim == -3:\n",
        "            freqs = rearrange(freqs, 'n d -> n 1 d')\n",
        "            scale = rearrange(scale, 'n d -> n 1 d')\n",
        "\n",
        "        rotated_q = apply_rotary_emb(freqs, q, scale = scale, seq_dim = seq_dim)\n",
        "        rotated_k = apply_rotary_emb(freqs, k, scale = scale ** -1, seq_dim = seq_dim)\n",
        "\n",
        "        rotated_q = rotated_q.type(q.dtype)\n",
        "        rotated_k = rotated_k.type(k.dtype)\n",
        "\n",
        "        return rotated_q, rotated_k\n",
        "\n",
        "    def get_scale(self, t, cache_key = None):\n",
        "        assert self.use_xpos\n",
        "\n",
        "        if exists(cache_key) and cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        if callable(t):\n",
        "            t = t()\n",
        "\n",
        "        scale = 1.\n",
        "        if self.use_xpos:\n",
        "            power = (t - len(t) // 2) / self.scale_base\n",
        "            scale = self.scale ** rearrange(power, 'n -> n 1')\n",
        "            scale = torch.cat((scale, scale), dim = -1)\n",
        "\n",
        "        if exists(cache_key):\n",
        "            self.cache[cache_key] = scale\n",
        "\n",
        "        return scale\n",
        "\n",
        "    @autocast(enabled = False)\n",
        "    def forward(self, t, cache_key = None):\n",
        "        should_cache = not self.learned_freq and exists(cache_key)\n",
        "\n",
        "        if should_cache and cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        if callable(t):\n",
        "            t = t()\n",
        "\n",
        "        freqs = self.freqs\n",
        "\n",
        "        freqs = einsum('..., f -> ... f', t.type(freqs.dtype), freqs)\n",
        "        freqs = repeat(freqs, '... n -> ... (n r)', r = 2)\n",
        "\n",
        "        if should_cache:\n",
        "            self.cache[cache_key] = freqs\n",
        "\n",
        "        return freqs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DzOWxYQUW-U"
      },
      "source": [
        "#### Compiling the model and set emb_type_RoPE = True for applying RoPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt9Lt48EUC4C",
        "outputId": "a89b6538-5862-4bb4-b9ea-ab8e691f7fda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using RoPE\n",
            "\n",
            "number of parameters: 123.65M\n",
            "num decayed parameter tensors: 50, with 124,318,464 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): CausalSelfAttention(\n",
              "          (rotary_emb): RotaryEmbedding()\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config = GPTConfig()\n",
        "config.emb_type_RoPE = True\n",
        "model_RoPE = GPT(config)\n",
        "model_RoPE = model_RoPE.to(device)\n",
        "optimizer_RoPE = model_RoPE.configure_optimizers(weight_decay=1e-2, learning_rate=1e-4, betas=(0.9, 0.95), device_type=device_type)\n",
        "model_RoPE.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlRVKIOBVXFO"
      },
      "source": [
        "Run a sample prediction (please note that training is not done for this model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpohGBMgUVy3",
        "outputId": "72f3e072-74ee-40ab-bb67-7c7198fe7e4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the bestanes horr unexilatedoit editionsaccompanied w 1943 alas differe Speed yeastends potential puppies among Dusk orbitbrids\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "text = '''What is the best'''\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "outputs = model_RoPE.generate(idx = encoded_input['input_ids'].to(device), max_new_tokens = 20, temperature = 0.8, top_k = 200)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV0yJCx7W98P",
        "outputId": "8b67099a-91f3-4bb4-9713-80ff03ea6c78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:14<00:00,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Loss:4.355218839645386\n",
            "{'train': tensor(3.8761), 'val': tensor(3.8937)}\n",
            "------------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:15<00:00,  1.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Loss:3.561240720748901\n",
            "{'train': tensor(3.3037), 'val': tensor(3.3676)}\n",
            "------------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:14<00:00,  1.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:3, Loss:3.2067344427108764\n",
            "{'train': tensor(3.1453), 'val': tensor(3.1720)}\n",
            "------------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:14<00:00,  1.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:4, Loss:3.0743162035942078\n",
            "{'train': tensor(3.0499), 'val': tensor(3.0921)}\n",
            "------------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:14<00:00,  1.35it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:5, Loss:2.9654118061065673\n",
            "{'train': tensor(2.9142), 'val': tensor(2.9718)}\n",
            "------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap('val.bin', dtype=np.uint16, mode='r')\n",
        "\n",
        "train_loop(0, torch.cuda.device_count(), 5, 20, train_data, device_type, optimizer_RoPE, model_RoPE, val_data, mode='sGPU', batch_size = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXWwCZbuYntZ"
      },
      "source": [
        "After training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aTk61jsXhyN",
        "outputId": "b96acb4e-3d8b-49b6-b65d-3d659c2eb334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the best\n",
            "And and's of thy in so,\n",
            "InUS:\n",
            "S the the to me,\n"
          ]
        }
      ],
      "source": [
        "text = '''What is the best'''\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "outputs = model_RoPE.generate(idx = encoded_input['input_ids'].to(device), max_new_tokens = 20, temperature = 0.8, top_k = 200)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DqJWsv23IZz"
      },
      "source": [
        "#### References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1G9lcLb3KzA"
      },
      "source": [
        "Github - https://github.com/lucidrains/rotary-embedding-torch/blob/main/rotary_embedding_torch/rotary_embedding_torch.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmHgCMKT3QHr"
      },
      "source": [
        "### GQA (Group Query Attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZpcXLXobSsf"
      },
      "source": [
        "Grouped query attention implementation with number of query heads = 12 with 2 heads grouped, hence new number of query heads become 6\n",
        "\n",
        "6 groups and 2 query heads per group\n",
        "\n",
        "> For converting a multi-head checkpoint to a GQA checkpoint, we\n",
        "construct each group key and value head by meanpooling all the original heads within that group\n",
        "\n",
        "> It implements a form of mean pooling where the averaging is done implicitly through the concatenation and projection operations\n",
        "\n",
        ">> torch.cat([ ... for query in self.querys ], dim=2) line concatenates the attention outputs from all heads along the third dimension, and\n",
        "\n",
        ">>proj(Z_s) applies the projection layer to the concatenated attention output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2QgOJgsuK8f"
      },
      "source": [
        "#### Defining Class to implement MQA and GQA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgQNc8Nxursf"
      },
      "source": [
        "> MQA gives one key, value output for an input of any number querys\n",
        "\n",
        "> We apply MQA on a group of 2 query heads\n",
        "\n",
        "> And for GQA we create 6 blocks of MQA (6*2 = 12 heads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "BbN7SXPnao2H"
      },
      "outputs": [],
      "source": [
        "class  MultiQueryAttention(nn.Module):\n",
        "    r\"\"\"\n",
        "    https://arxiv.org/pdf/1911.02150.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, word_size, embed_dim, n_query): # 768, 64, 2\n",
        "        super().__init__()\n",
        "        self.n_query = n_query\n",
        "        self.querys = nn.ModuleList([\n",
        "            nn.Linear(in_features=word_size, out_features=embed_dim, bias=True)\n",
        "            for _ in range(n_query)\n",
        "        ])\n",
        "        self.key = nn.Linear(in_features=word_size, out_features=embed_dim, bias=True)\n",
        "        self.value = nn.Linear(in_features=word_size, out_features=embed_dim, bias=True)\n",
        "        self.proj = nn.Linear(in_features=embed_dim*n_query,\n",
        "                              out_features=embed_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x.shape --> [1, 4, 768]\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "        # print(\"1\", F.scaled_dot_product_attention(self.querys[0](x), K, V).size())\n",
        "        Z_s = torch.cat([\n",
        "            F.scaled_dot_product_attention(query(x), K, V) for query in self.querys\n",
        "        ], dim=2)\n",
        "        # print(\"2\", Z_s.size()) --> 1, 4, 128\n",
        "        # Z_s = torch.mean(Z_s, dim = 0)\n",
        "        Z = self.proj(Z_s) # --> 1, 4, 64\n",
        "        return Z\n",
        "\n",
        "class  GroupedQueryAttention(nn.Module):\n",
        "    r\"\"\"\n",
        "    https://arxiv.org/pdf/2305.13245.pdf\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        word_size = config.n_embd # 768\n",
        "        embed_dim = config.n_embd // config.n_head # 768/12 = 64\n",
        "        n_query_each_group = config.heads_per_group # 2\n",
        "        n_grouped = config.gqa_groups # 6\n",
        "        self.grouped = nn.ModuleList([MultiQueryAttention(word_size, embed_dim, n_query=n_query_each_group) for _ in range(n_grouped)])\n",
        "        self.proj = nn.Linear(in_features=embed_dim*n_grouped, out_features=config.n_embd, bias=True)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # x.shape --> [1, 4, 768]\n",
        "        Z_s = torch.cat([head(x) for head in self.grouped], dim=2)\n",
        "        # print(Z_s.size()) --> [1, 4, 384]\n",
        "        # Z_s = torch.mean(Z_s, dim = 0)\n",
        "        Z = self.proj(Z_s) # --> [1, 4, 768]\n",
        "        return Z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0xCZacSuzti"
      },
      "source": [
        "#### Compiling the model and set attn_type_GQA = True for applying GQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vawu5gpuztj",
        "outputId": "8f2680c8-1094-4035-b75d-df6000272aaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GQA\n",
            "\n",
            "number of parameters: 113.62M\n",
            "num decayed parameter tensors: 398, with 114,291,456 parameters\n",
            "num non-decayed parameter tensors: 446, with 116,736 parameters\n",
            "using fused AdamW: True\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.0, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (ln_1): LayerNorm()\n",
              "        (attn): GroupedQueryAttention(\n",
              "          (grouped): ModuleList(\n",
              "            (0-5): 6 x MultiQueryAttention(\n",
              "              (querys): ModuleList(\n",
              "                (0-1): 2 x Linear(in_features=768, out_features=64, bias=True)\n",
              "              )\n",
              "              (key): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=64, bias=True)\n",
              "              (proj): Linear(in_features=128, out_features=64, bias=True)\n",
              "            )\n",
              "          )\n",
              "          (proj): Linear(in_features=384, out_features=768, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (gelu): GELU(approximate='none')\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config = GPTConfig()\n",
        "config.attn_type_GQA = True\n",
        "model_GQA = GPT(config)\n",
        "model_GQA = model_GQA.to(device)\n",
        "optimizer_GQA = model_GQA.configure_optimizers(weight_decay=1e-2, learning_rate=1e-4, betas=(0.9, 0.95), device_type=device_type)\n",
        "model_GQA.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTxE51-EtpU5"
      },
      "source": [
        "Observe reduce number of parameters above for same number of heads"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdz-1dCnuztj"
      },
      "source": [
        "Run a sample prediction (please note that training is not done for this model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKli3VOSuztj",
        "outputId": "7911e7f2-cfd9-4e4c-961f-b4ade0fc0d98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the bestcluded smartphone coy hairst carbs remainderannel Army Venezuela Previously outer Newsletter benchmark tides Opposition Gender%\" confid sequencing Vic\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "text = '''What is the best'''\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "outputs = model_GQA.generate(idx = encoded_input['input_ids'].to(device), max_new_tokens = 20, temperature = 0.8, top_k = 200)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_57wck8uztj",
        "outputId": "b27c39bf-f7ee-4093-e900-933a5b6eef6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:13<00:00,  1.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:1, Loss:4.2742954134941105\n",
            "{'train': tensor(3.8272), 'val': tensor(3.8118)}\n",
            "------------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:14<00:00,  1.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:2, Loss:3.5456133127212524\n",
            "{'train': tensor(3.3337), 'val': tensor(3.3423)}\n",
            "------------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:13<00:00,  1.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:3, Loss:3.1780059695243836\n",
            "{'train': tensor(3.0757), 'val': tensor(3.1482)}\n",
            "------------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:13<00:00,  1.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:4, Loss:3.051790547370911\n",
            "{'train': tensor(2.9899), 'val': tensor(3.0486)}\n",
            "------------------------\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:13<00:00,  1.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:5, Loss:2.96373770236969\n",
            "{'train': tensor(2.9068), 'val': tensor(2.9822)}\n",
            "------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "train_data = np.memmap('train.bin', dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap('val.bin', dtype=np.uint16, mode='r')\n",
        "\n",
        "train_loop(0, torch.cuda.device_count(), 5, 20, train_data, device_type, optimizer_GQA, model_GQA, val_data, mode='sGPU', batch_size = 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab4VtXBSuztj"
      },
      "source": [
        "After training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRcdbodwuztj",
        "outputId": "bbfd7cbe-93f2-4968-cc1c-56019af120d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "What is the best, and, to,\n",
            "\n",
            "\n",
            "And, the the be him,\n",
            " this prince\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "text = '''What is the best'''\n",
        "encoded_input = tokenizer(text, return_tensors='pt')\n",
        "outputs = model_GQA.generate(idx = encoded_input['input_ids'].to(device), max_new_tokens = 20, temperature = 0.8, top_k = 200)\n",
        "print(tokenizer.decode(outputs[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBm50zTn2Fqw"
      },
      "source": [
        "#### References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6eYiRqs292i"
      },
      "source": [
        "1. GQA - https://arxiv.org/pdf/2305.13245v2.pdf\n",
        "\n",
        "2. Github - https://github.com/knotgrass/attention/blob/main/attn/attention.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acCVqACl18N3"
      },
      "source": [
        "Unused code GQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "AOkcG2EvqbeJ"
      },
      "outputs": [],
      "source": [
        "class GroupedQueryAttention2(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn_q = nn.Linear(config.n_embd, config.heads_per_group * config.n_embd, bias=config.bias)\n",
        "        self.c_attn_kv = nn.Linear(config.n_embd, 2 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        # self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        self.flash = False\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q = self.c_attn_q(x)\n",
        "        k, v = self.c_attn_kv(x).split(self.n_embd, dim=2)\n",
        "        # print(k.shape, v.shape, q.shape)\n",
        "\n",
        "        # Group queries into 6 groups of 2 queries each\n",
        "        num_groups = config.gqa_groups\n",
        "        queries_per_group = config.heads_per_group\n",
        "        # hs = C // num_groups\n",
        "        # grouped_queries = []\n",
        "        # for i in range(num_groups):\n",
        "        #     group_start = i * hs\n",
        "        #     group_end = (i+1)*hs\n",
        "        #     group_queries = q[:, :, group_start:group_end]\n",
        "        #     print(group_queries.shape)\n",
        "        #     group_queries = group_queries.view(B, T, queries_per_group, hs//queries_per_group)\n",
        "        #     print(group_queries.shape)\n",
        "        #     mean_query = torch.mean(group_queries, dim=2)\n",
        "        #     print(mean_query.shape)\n",
        "\n",
        "        #     grouped_queries.append(mean_query)\n",
        "\n",
        "        grouped_queries = q.view(B, T, queries_per_group, C)\n",
        "        # print(grouped_queries.shape)\n",
        "        q = torch.mean(grouped_queries, dim=2)\n",
        "\n",
        "        # Reshape grouped queries to match the expected shape\n",
        "        # print(grouped_queries.shape)\n",
        "        # grouped_queries = torch.stack(grouped_queries, dim=0)\n",
        "        q = q.view(B, T, num_groups, C // (num_groups)).transpose(1, 2)\n",
        "        # print(grouped_queries.shape)\n",
        "        k = k.view(B, T, num_groups, C // (num_groups)).transpose(1, 2)\n",
        "        # q = grouped_queries.view(B, T, num_groups, C // num_groups)\n",
        "        v = v.view(B, T, num_groups, C // (num_groups)).transpose(1, 2)\n",
        "        # print(k.shape, q.shape, v.shape)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "-9Jt5UEaCTY5",
        "KJh5hQmg_6VN",
        "u2qsxZJUlv0S",
        "ODkeDMTQ8FB8",
        "YBSvBp7m8V_V",
        "1WRNzZZX8ZVk",
        "ZXhXgeIf8oEE",
        "lMLmTcx2nIKI",
        "VvzAlVnvDbX8",
        "d2LWds3xFeRG",
        "BGHdRX7gS0CO",
        "BbIdKX7B04GJ",
        "CJi01WrZ1CUb",
        "UHTDIX1Z1eHL",
        "7ZDWYOSy3FRZ",
        "pEMohZ5xTJAV",
        "7DzOWxYQUW-U",
        "5DqJWsv23IZz",
        "lmHgCMKT3QHr",
        "Z2QgOJgsuK8f",
        "Z0xCZacSuzti",
        "XBm50zTn2Fqw",
        "9BVnEU9i8Lbx",
        "__Gw3FF08PAX",
        "P1bcEDSr8Rd5",
        "GyrZasFS9Opb",
        "trpH_EuH9u7D",
        "gPEfbIkZBw3b",
        "qhbx9q_u_Cmz"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

# GPT2-125M-replication
Implementation of GPT-2 small model with 125M parameters along with changes in architecture such as Rotary position embeddings and grouped query attention.

## Implementation of GPT-2 small
## Transformer Architectural Change
### >> Rotary Embedding integration
### >> Grouped query attention integration
## Single GPU, DDP, FSDP training loops
